{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>CS4618: Artificial Intelligence I</h1>\n",
    "<h1>Datasets</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy.random import rand\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Features</h1>\n",
    "<ul>\n",
    "    <li>Suppose we want to store data about objects, such as houses.</li>\n",
    "    <li><b>Features</b> describe the houses, e.g.\n",
    "        <ul>\n",
    "            <li>$\\mathit{flarea}$: the total floor area (in square metres);</li>\n",
    "            <li>$\\mathit{bdrms}$: the number of bedrooms;</li>\n",
    "            <li> $\\mathit{bthrms}$: the number of bathrooms.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>A particular house has <b>values</b> for the features:\n",
    "        <ul>\n",
    "            <li>e.g. your house: $\\mathit{flarea} = 126, \\mathit{bdrms} = 3, \\mathit{bthrms} = 1$.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Then we can represent a house using a vector:\n",
    "        <ul>\n",
    "            <li>e.g. your house: $\\cv{126\\\\3\\\\1}$\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We will always use $n$ to refer to the number of features, e.g. above $n = 3$.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Examples</h1> \n",
    "<ul>\n",
    "    <li>Suppose we collect a <b>dataset</b> containing data about lots of houses, e.g.:\n",
    "        $$\\cv{126\\\\3\\\\1} \\,\\, \\cv{92.9\\\\3\\\\2} \\,\\,\\cv{171.9\\\\4\\\\3} \\,\\, \\cv{79\\\\3\\\\1}$$\n",
    "    </li>\n",
    "    <li>Each member of this dataset is called an <b>example</b>, and we will use $m$ to refer to the number of examples, e.g.\n",
    "        above $m = 4$.\n",
    "    </li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Dataset notation</h1>\n",
    "<ul>\n",
    "    <li>We will use a <em>superscript</em> to index the examples.\n",
    "        <ul>\n",
    "            <li>\n",
    "                $\\v{x}^{(i)}$ will be the $i$th example.\n",
    "            </li>\n",
    "            <li>\n",
    "                The first example in the dataset is $\\v{x}^{(1)}$, the second is $\\v{x}^{(2)}$, $\\ldots$, \n",
    "                the last is $\\v{x}^{(m)}$ (Note, we index from 1.)\n",
    "            </li>\n",
    "            <li>\n",
    "                We're writing the superscript in parentheses to make it clear that we are using it for indexing.\n",
    "                It is not 'raising to a power'. If we want to raise to a power, we will drop the parentheses.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We will use a <em>subscript</em> to index the features (again starting from 1).</li>\n",
    "    <li>Class exercise. Using the dataset from above:\n",
    "        <ul>\n",
    "            <li>what is $\\v{x}_2^{(1)}$?</li>\n",
    "            <li>what is $\\v{x}_1^{(2)}$?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Dataset as a matrix</h1>\n",
    "<ul>\n",
    "    <li>We can represent a dataset $\\Set{\\v{x}^{(1)}, \\v{x}^{(2)}, \\ldots, \\v{x}^{(m)}}$ as a $m \\times n$\n",
    "        matrix $\\v{X}$ as follows:\n",
    "        $$\\v{X} = \\begin{bmatrix}\n",
    "              \\v{x}_1^{(1)} & \\v{x}_2^{(1)} & \\ldots & \\v{x}_n^{(1)} \\\\\n",
    "              \\v{x}_1^{(2)} & \\v{x}_2^{(2)} & \\ldots & \\v{x}_n^{(2)} \\\\\n",
    "              \\vdots        & \\vdots        & \\vdots & \\vdots \\\\\n",
    "              \\v{x}_1^{(m)} & \\v{x}_2^{(m)} & \\ldots & \\v{x}_n^{(m)} \\\\\n",
    "              \\end{bmatrix}\n",
    "        $$\n",
    "    </li>\n",
    "    <li>Note how each example becomes a <em>row</em> in $\\v{X}$.</li>\n",
    "    <li>You can think of row $i$ as the transpose of $\\v{x}^{(i)}$.</li>\n",
    "    <li>For the example dataset, we get:\n",
    "        $$\\v{X} = \n",
    "            \\begin{bmatrix}\n",
    "                126 & 3 & 1 \\\\\n",
    "                92.9 & 3 & 2 \\\\\n",
    "                171.9 & 4 & 3 \\\\\n",
    "                79 & 3 & 1\n",
    "            \\end{bmatrix}\n",
    "        $$\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Cork Property Prices Dataset</h1>\n",
    "<ul>\n",
    "    <li>In August 2019, I scraped a dataset of property prices for Cork city from www.daft.ie.</li>\n",
    "    <li>They are in a CSV file. Each line in the file is an example, representing one house.</li>\n",
    "    <li>Hence, each line of the file contains the feature-values for the floor area, number of bedrooms, number of\n",
    "        bathrooms, and several other features that we will ignore for now.\n",
    "    </li>\n",
    "    <li>We will use the pandas library:\n",
    "        <ul>\n",
    "            <li>to read the dataset from the csv file into what pandas calls a DataFrame;</li>\n",
    "            <li>to explore the dataset: looking at values and computing summary statistics.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Then we will extract some of the features (columns) and convert to a numpy 2D array, before using the data\n",
    "        to find houses similar to yours.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Using pandas to Read and Explore the Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"../datasets/dataset_corkA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(464, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dimensions\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['flarea', 'bdrms', 'bthrms', 'price'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The features\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flarea    float64\n",
       "bdrms       int64\n",
       "bthrms      int64\n",
       "price       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The datatypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 464 entries, 0 to 463\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   flarea  464 non-null    float64\n",
      " 1   bdrms   464 non-null    int64  \n",
      " 2   bthrms  464 non-null    int64  \n",
      " 3   price   464 non-null    int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 14.6 KB\n"
     ]
    }
   ],
   "source": [
    "# The columns and datatypes (again) but also whether there are any nulls in the dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flarea</th>\n",
       "      <th>bdrms</th>\n",
       "      <th>bthrms</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>464.000000</td>\n",
       "      <td>464.000000</td>\n",
       "      <td>464.000000</td>\n",
       "      <td>464.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>125.460151</td>\n",
       "      <td>3.329741</td>\n",
       "      <td>2.120690</td>\n",
       "      <td>352.297414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>70.692202</td>\n",
       "      <td>1.068445</td>\n",
       "      <td>1.061033</td>\n",
       "      <td>197.464495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>95.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>82.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>235.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>110.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>295.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>140.600000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>395.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>575.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1495.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           flarea       bdrms      bthrms        price\n",
       "count  464.000000  464.000000  464.000000   464.000000\n",
       "mean   125.460151    3.329741    2.120690   352.297414\n",
       "std     70.692202    1.068445    1.061033   197.464495\n",
       "min     40.000000    1.000000    1.000000    95.000000\n",
       "25%     82.000000    3.000000    1.000000   235.000000\n",
       "50%    110.000000    3.000000    2.000000   295.000000\n",
       "75%    140.600000    4.000000    3.000000   395.000000\n",
       "max    575.000000    9.000000    6.000000  1495.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary statistics\n",
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flarea</th>\n",
       "      <th>bdrms</th>\n",
       "      <th>bthrms</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>111.9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120.8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flarea  bdrms  bthrms  price\n",
       "0   111.9      3       3    305\n",
       "1    95.0      3       3    255\n",
       "2   120.8      3       3    275"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A few of the examples\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Convert to a numpy 2D array</h2>\n",
    "<ul>\n",
    "    <li>We will select certain features (columns) from the pandas DataFrame\n",
    "        and convert to a 2D numpy array\n",
    "    </li>\n",
    "    <li>(Later in the module, we will use a <code>ColumnTransformer</code> to do this.)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features we want to select\n",
    "features = [\"flarea\", \"bdrms\", \"bthrms\"]\n",
    "\n",
    "# Extract these features and convert to numpy 2D array\n",
    "X = df[features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[111.9,   3. ,   3. ],\n",
       "       [ 95. ,   3. ,   3. ],\n",
       "       [120.8,   3. ,   3. ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at a few rows in X - to show you that we now have a 2D numpy array\n",
    "X[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Similarity &amp; Distance</h1>\n",
    "<ul>\n",
    "    <li>In AI, we often want to know how <em>similar</em> one object is to another.\n",
    "        <ul>\n",
    "            <li>E.g. how similar is my house to yours?</li>\n",
    "            <li>E.g. which house in our dataset is most similar to yours?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>In fact, here we are instead going to measure how <em>different</em> they are using a <b>distance function</b>.\n",
    "        <ul>\n",
    "            <li>(N.B. This is not about geographical distance.)</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Let $\\v{x}$ be one vector of feature values and $\\v{x}'$ be another.</li>\n",
    "    <li>Simplest is to measure their <b>Euclidean distance</b>:\n",
    "        $$d(\\v{x}, \\v{x}') = \\sqrt{(\\v{x}_1 - \\v{x}_1')^2 + (\\v{x}_2 - \\v{x}_2')^2 + \\ldots + (\\v{x}_n - \\v{x}_n')^2}$$\n",
    "        or, more concisely:\n",
    "        $$d(\\v{x}, \\v{x}') = \\sqrt{\\sum_{j=1}^n(\\v{x}_j - \\v{x}_j')^2}$$\n",
    "    </li>\n",
    "    <li>Euclidean distance has a minimum value of 0 (meaning identical) but no maximum value (depends on your data).</li>\n",
    "    <li>Class exercise. What is the Euclidean distance between $\\v{x} = \\cv{100\\\\1\\\\4}$ and $\\v{x}' = \\cv{100\\\\5\\\\1}$?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Euclidean Distance in numpy</h2>\n",
    "<ul>\n",
    "    <li>It has a nice vectorized implementation (no loop!) using numpy:</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euc(x, xprime):\n",
    "    return np.sqrt(np.sum((x - xprime)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.026297590440446"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "your_house = np.array([126.0, 3, 1])\n",
    "my_house = np.array([107.0, 2, 1])\n",
    "\n",
    "euc(your_house, my_house)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>We can compute the distance between your house and all the houses in X.</li>\n",
    "    <li>(We have to write a loop here, because our <code>euc</code> function is not a vectorized function itself.)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = [euc(your_house, x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14.241137595009741, 31.064449134018133, 5.571355310873651]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just to show you, here are the first 3 distances\n",
    "dists[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0332473082471605"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Even better, we can, with one line of code, find the most similar house\n",
    "np.min([euc(your_house, x) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Even better again, we can find which house is the most similar\n",
    "np.argmin([euc(your_house, x) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flarea    125.74\n",
       "bdrms       3.00\n",
       "bthrms      2.00\n",
       "price     398.00\n",
       "Name: 196, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best of all, we can display the most similar house\n",
    "df.iloc[np.argmin([euc(your_house, x) for x in X])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Problems with Euclidean distance</h1>\n",
    "<ul>\n",
    "    <li>There are at least three problems with Euclidean distance (and many other distance measures too):\n",
    "        <ul>\n",
    "            <li>Features with different scales;</li>\n",
    "            <li>Features that are correlated with each other;</li>\n",
    "            <li>The curse of dimensionality.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Scaling Numeric Values</h1>\n",
    "<ul>\n",
    "    <li>Different numeric-valued features often have very different ranges.\n",
    "        <ul>\n",
    "            <li>E.g. the values for floor area are going to range from a few tens to a few hundreds of square metres.</li>\n",
    "            <li>But the number of bedrooms and bathrooms is going to range from 0 to a dozen or so at most.\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        When computing the Euclidean distance, features with large ranges will dominate the distance calculations, \n",
    "        thus giving features with small ranges negligible influence.\n",
    "    </li>\n",
    "    <li>\n",
    "        E.g., consider your house $\\v{x} = \\cv{126\\\\3\\\\1}$ and two others, $\\v{y} = \\cv{131\\\\3\\\\1}$ and\n",
    "        $\\v{z} = \\cv{126\\\\7\\\\1}$. \n",
    "        <ul>\n",
    "            <li><em>Intuitively</em>, which house is more similar to yours, $\\v{y}$ or $\\v{z}$?</li>\n",
    "            <li>Now compute the Euclidean distances.</li>\n",
    "            <li>According to these distances, which house is more similar to yours?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        The solution is to <b>scale</b> (or 'normalize') the values so that they have similar ranges.\n",
    "    </li>\n",
    "    <li>There are several ways to do this. One is <b>min-max scaling</b>, but the one we'll discuss is <b>standardization</b>.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Standardization</h2>\n",
    "<ul>\n",
    "    <!--\n",
    "    <li>In some cases, you don't want feature values to have the same range but to have the same mean\n",
    "        and even the same variance\n",
    "    </li>\n",
    "    -->\n",
    "    <li>\n",
    "        One idea is <b>mean centering</b>, where you subtract the mean value of the feature.\n",
    "        <ul>\n",
    "            <li>If you do this to all values, some of the new values will be positive and some will be negative and \n",
    "                their mean will be approximately zero.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>But better still is <b>standardization</b>, in which you subtract the mean and divide by the standard\n",
    "        deviation:\n",
    "        $$\\v{x}_j \\gets \\frac{\\v{x}_j - \\mu_j}{\\sigma_j}$$\n",
    "        where $\\mu_j$ is the mean of the values for feature $j$ and $\\sigma_j$ is their standard deviation\n",
    "    </li>\n",
    "    <li>\n",
    "        If you use this, then the mean will be approximately zero, the standard deviation will be 1.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Standardization in scikit-learn</h2>\n",
    "<ul>\n",
    "    <li>scikit-learn provides a class called <code>StandardScaler</code>.\n",
    "    </li>\n",
    "    <li>It uses means and standard deviations that it calculates from your dataset. (Statisticians would say that it should\n",
    "        use the population mean and standard deviation, but these are generally not known.)\n",
    "    </li>\n",
    "    <li>We create the scaler and then run its <code>fit</code> and <code>transform</code> methods.</li>\n",
    "    <li>(Later in the module, when we are using a <code>ColumnTransformer</code>, running these methods\n",
    "        will be done for us.)\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.19202665, -0.30895098,  0.82962481],\n",
       "       [-0.43134924, -0.30895098,  0.82962481],\n",
       "       [-0.06599286, -0.30895098,  0.82962481]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at a few rows in X\n",
    "X_scaled[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00764486, -0.30895098, -1.05736495])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's scale your house too\n",
    "# Don't try to understand or copy this code - it's a hack that you won't need\n",
    "your_house = np.array([[126.0, 3, 1]])\n",
    "your_house_scaled = scaler.transform(your_house)[0]\n",
    "your_house_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see what effect this has had, let's see which house is most similar to yours\n",
    "np.argmin([euc(your_house_scaled, x) for x in X_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flarea    122.4\n",
       "bdrms       3.0\n",
       "bthrms      1.0\n",
       "price     295.0\n",
       "Name: 328, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[np.argmin([euc(your_house_scaled, x) for x in X_scaled])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Features that are Correlated</h1>\n",
    "<ul>\n",
    "    <li>Let's start with an extreme example. \n",
    "        <ul>\n",
    "            <li>Suppose one feature is the floor area in square metres and\n",
    "                another is the floor area in square feet.\n",
    "                Then it's clear that, even after scaling, when calculating distances, floor area will have greater\n",
    "                influence than other features, such as the number of bedrooms, because it is in the dataset twice.\n",
    "            </li>\n",
    "            <li>Examples are often less stark. For example, floor area and the number of bedrooms are correlated,\n",
    "                and so their contributions to the distance calculations are not independent of each other.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Ideally the features should be independent (at least, linearly independent).</li>\n",
    "    <li>Yet, few people who use distances do anything about this problem!</li>\n",
    "    <li>Solutions (which we're not covering in detail) include, e.g. (a) feature weighting and (b) projections to\n",
    "        a new feature space whose features are (linearly) independent (e.g. using Principal Component\n",
    "        Analysis).\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>The Curse of Dimensionality</h1>\n",
    "<ul>\n",
    "    <li>In some datasets, examples have thousands or even millions of features.\n",
    "        <ul>\n",
    "            <li>E.g. datasets from astronomy;</li>\n",
    "            <li>E.g. datasets of images and videos;</li>\n",
    "            <li>E.g. datasets of documents where each unique word is a feature.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Is it better or worse to have more features?\n",
    "        <ul>\n",
    "            <li>Storage and processing costs increase.</li>\n",
    "            <li>Apart from efficiency, intuitively, more features is better:\n",
    "                <ul>\n",
    "                    <li>e.g. describing houses more completely.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>But, counter-intuitively, that isn't true in general.\n",
    "                <ul>\n",
    "                    <li>As the number of features grows, algorithms that use distance and density, will find it harder \n",
    "                        to find good solutions.\n",
    "                    </li>\n",
    "                    <li>The problems that arise as the number of features grows have been called <b>the curse of dimensionality</b>.\n",
    "    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Example of the Curse of Dimensionality</h2>\n",
    "<ul>\n",
    "    <li>The code that follows (which you don't need to study):\n",
    "        <ul>\n",
    "            <li>generates a random dataset where $m = 400$ and $n = 2$ and both features have values in $[0, 1)$;\n",
    "            </li>\n",
    "            <li>computes the Euclidean distance between all pairs of examples;</li>\n",
    "            <li>finds $d_{\\mathit{min}}$, the smallest of these distances;</li>\n",
    "            <li>finds $d_{\\mathit{max}}$, the largest of the distances;</li>\n",
    "            <li>computes the ratio $\\frac{d_{\\mathit{max}}}{d_{\\mathit{min}}}$.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>It then does this all again but with $n = 3, 4, 5,\\ldots,500$.</li>\n",
    "    <li>Then it plots the ratios that it has computed ($y$-axis, but note its scale) against $n$ ($x$-axis).</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = 400\n",
    "n_range = range(1, 500)\n",
    "\n",
    "ratios = []\n",
    "for n in n_range:\n",
    "    X = rand(m, n)\n",
    "    dists = euclidean_distances(X)\n",
    "    non_zero_dists = dists[dists > 0]\n",
    "    ratios += [np.max(non_zero_dists) / (np.min(non_zero_dists))]\n",
    "    \n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.title(\"Ratio of max distance to min distance in datasets with ever more features\")\n",
    "plt.scatter(n_range, ratios)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"n\")\n",
    "plt.xlim(0, 500)\n",
    "plt.ylabel(\"ratio\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>As $n \\rightarrow \\infty$, $d_{\\mathit{max}} \\rightarrow d_{\\mathit{min}}$, so their ratio tends to 1.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.2464041377283335,\n",
       " 1.2484055153470177,\n",
       " 1.239181780515684,\n",
       " 1.2644094317281307,\n",
       " 1.290569351360222]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since it may not be clear from the graph, we'll show the last 5 of the ratios that it calculated\n",
    "ratios[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>We conclude (counter-intutively) that examples become equi-distant!</li>\n",
    "    <li>This obviously undermines methods that depend on finding objects that are similar to each other, as we were\n",
    "        doing earlier &mdash; with more features, the most similar object becomes more arbitrary!\n",
    "    </li>\n",
    "    <li>The problem extends to other distance/similarity measures, e.g. cosine similarity.</li>\n",
    "    <li>Fortunately, there are lots of methods available for reducing dimensionality.\n",
    "        One solution is to retain the principle components found by Principal Component Analysis. This is\n",
    "        interesting because PCA was suggested above as a solution to the problem of correlated features. \n",
    "        It can actually help us solve both problems.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
