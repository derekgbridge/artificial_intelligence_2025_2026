{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CS4618: Artificial Intelligence I</h1>\n",
    "<h1>Training a Neural Network</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import exp\n",
    "\n",
    "from keras.datasets.mnist import load_data\n",
    "\n",
    "from keras import Model\n",
    "from keras import Input\n",
    "from keras.layers import Rescaling\n",
    "from keras.layers import Dense\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "\n",
    "# Load MNIST into four Numpy arrays\n",
    "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = load_data()\n",
    "mnist_x_train = mnist_x_train.reshape((60000, 28 * 28))\n",
    "mnist_x_test = mnist_x_test.reshape((10000, 28 * 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "<h1>Acknowledgements</h1>\n",
    "<ul>\n",
    "    <li>The <code>LearningRateFinder</code> class comes from Adrian Rosebrock's excellent\n",
    "        <a href=\"https://www.pyimagesearch.com/\">pyimagesearch</a> site. He credits the\n",
    "        <a href=\"http://github.com/amaiya/ktrain\">ktrain library</a> for the original code.\n",
    "    </li>\n",
    "    <li>The <code>CyclicLR</code> class comes from Brad Kenstler and others:\n",
    "        <a href=\"https://github.com/bckenstler/CLR/blob/master/clr_callback.py\">https://github.com/bckenstler/CLR/blob/master/clr_callback.py</a>.\n",
    "    </li>\n",
    "    <li>They both credit the following paper for both ideas:<br />\n",
    "        L. N. Smith, \"Cyclical Learning Rates for Training Neural Networks,\" \n",
    "        2017 IEEE Winter Conference on Applications of Computer Vision (WACV), \n",
    "        Santa Rosa, CA, 2017, pp. 464-472, doi: 10.1109/WACV.2017.58.\n",
    "    </li>\n",
    "</ul>\n",
    "-->\n",
    "<h1>Introduction</h1>\n",
    "<ul>\n",
    "    <li>We'll give an overview of Gradient Descent for neural networks.\n",
    "        <ul>\n",
    "            <li>Our goal is to get the 'flavour' of the algorithm.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Then, we will discuss the Vanishing Gradients Problem and its solutions.</li>\n",
    "    <li>Finally, we'll discuss how to choose a learning rate.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Gradient Descent for Neural Networks</h1>\n",
    "<ul>\n",
    "    <li>Let's start by reminding ourselves of (Batch) Gradient Descent for OLS regression:\n",
    "        <ul style=\"background: lightgrey; list-style: none\">\n",
    "            <li>initialize $\\v{\\beta}$ randomly\n",
    "            <li>\n",
    "                repeat until convergence\n",
    "                <ul>\n",
    "                    <li>\n",
    "                        $\\v{\\beta} \\gets \\v{\\beta} - \\frac{\\alpha}{m}\\v{X}^T(\\v{X}\\v{\\beta} - \\v{y})$\n",
    "                    </li>\n",
    "                </ul>\n",
    "             </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We see it making predictions $\\hat{\\v{y}} = \\v{X}\\v{\\beta}$ for all the examples $\\v{X}$ and \n",
    "        comparing them with target values, $\\v{y}$.\n",
    "    </li>\n",
    "    <li>And we see it updating all the parameters $\\v{\\beta}$ by an amount equal to the negative of\n",
    "        the gradients, multiplied by the learning rate $\\alpha$.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Backpropagation</h1>\n",
    "<ul>\n",
    "    <li>For neural networks, however, there is a problem.\n",
    "        <ul>\n",
    "            <li>At the output layer, we can straightforwardly compute loss: we can get the network's output\n",
    "                (its prediction) and we have the target value, because the training set is a labeled dataset.\n",
    "            </li>\n",
    "            <li>But we cannot straightforwardly compute loss at the hidden layers: we know what outputs their\n",
    "                neurons produce but we do not know what they should produce (target values). The labeled\n",
    "                dataset doesn't tell us the target values for hidden layer neurons.\n",
    "            </li>\n",
    "        </ul>  \n",
    "    </li>\n",
    "    <li>The solution is to assume that each neuron in layer $l$ is responsible for some part of the loss\n",
    "        of the neurons it is connected to in layer $l+1$. We'll refer to this amount as the 'error signal'.\n",
    "    </li>\n",
    "    <li>This leads to the idea of an algorithm that makes two passes through the network:\n",
    "        <ul>\n",
    "            <li>a <b>forward pass</b> to make predictions: we feed training set $\\v{X}$ in to the network and then work forwards\n",
    "                through the network, computing \n",
    "                activations layer by layer\n",
    "                until we reach the output layer;\n",
    "            </li>\n",
    "            <li>a <b>backward pass</b> to compute the gradients: we calculate loss at the output layer and then\n",
    "                work backwards through the network computing error signals layer by layer until we reach the\n",
    "                input layer.\n",
    "            </li>\n",
    "        </ul>\n",
    "        With these two steps completed, we have all the gradients, so we can update all the weights and biases.\n",
    "    </li>\n",
    "    <li>This very informal description helps you see why the calculation of the gradients (and sometimes\n",
    "        the entire algorithm) is referred to as <b>backpropagation</b> (or just <b>backprop</b>).\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Demo</h2>\n",
    "<ul>\n",
    "    <li>We'll take a look at <a href=\"http://experiments.mostafa.io/public/ffbpann/\">http://experiments.mostafa.io/public/ffbpann/</a>\n",
    "    </li>\n",
    "    <li>Or we'l take a look at <a href=\"https://mlu-explain.github.io/neural-networks/\">https://mlu-explain.github.io/neural-networks/</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The Backpropagation Algorithm</h2>\n",
    "<ul>\n",
    "    <li><b>Random initialization</b>: initialize all weights and biases randomly</li>\n",
    "    <li><b>Forward propagation</b>: make predictions for all the training examples:\n",
    "        <ul>\n",
    "            <li>Layer by layer from from layer 1 to layer $L$:\n",
    "                <ul>\n",
    "                    <li>Calculate the inputs to the units in that layer (weighted sums plus biases)</li>\n",
    "                    <li>Calculate the outputs of the units in that layer (using activation function)</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Backpropagation</b>:\n",
    "        <ul>\n",
    "            <li>Calculate the error signals $\\Delta$ at layer $L$</li>\n",
    "            <li>Layer by layer in reverse from layer $L-1$ to layer 1:\n",
    "                <ul>\n",
    "                    <li>Calculate the error signals $\\Delta$ for the units in that layer</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Update all the weights</b>: \n",
    "        $w^{(l)}_{i,j} \\gets w^{(l)}_{i,j} - \\alpha \\times a_i \\times \\Delta^{(l)}_j$\n",
    "     </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Class exercise</h2>\n",
    "<ul>\n",
    "    <li>Back prop starts by initializing weights randomly.\n",
    "        <ul>\n",
    "            <li>E.g it was common to use a normal distribution with mean of 0 and standard deviation \n",
    "                of, e.g., 0.05.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>A novice proposes instead to initialize them all to zero.</li>\n",
    "    <li>Why in general does this not make sense?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The Backpropagation Algorithm (Vectorized)</h2>\n",
    "<ul>\n",
    "    <li><b>Random initialization</b>: initialize all weights randomly</li>\n",
    "    <li><b>Forward propagation</b>:\n",
    "        <ul>\n",
    "            <li>Calculate and store $\\v{Z}^{(1)} = \\v{X}\\v{W}^{(1)}$\n",
    "            <li>Layer by layer from layer $l=1$ to layer $L$:\n",
    "                <ul>\n",
    "                    <li>Calculate and store $\\v{A}^{(l)} = g^{(l)}(\\v{Z}^{(l)})$</li>\n",
    "                    <li>Calculate $\\v{Z}^{(l+1)} = \\v{A}^{(l)}\\v{W}^{(l+1)}$</li>\n",
    "                    <li>Calculate and store $\\v{G}^{(l)} = g'(\\v{Z}^{(l)})^T$</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Backpropagation</b>:\n",
    "        <ul>\n",
    "            <li>Calculate $\\v{D}^{(L)} = (\\v{A}^{(L)} âˆ’ \\v{Y})^T$</li>\n",
    "            <li>Layer by layer in reverse from layer $l=L-1$ to layer 1:\n",
    "                <ul>\n",
    "                    <li>Calculate and store $\\v{D}^{(l)} = \\v{G}^{(l)} * \\v{W}^{(l)}\\v{D}^{(l+1)}$</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Update all the weights</b>:\n",
    "        <ul>\n",
    "            <li>$\\v{W}^{(1)} = \\v{W}^{(1)} - \\alpha (\\v{D}^{(1)}\\v{X})^T$</li>\n",
    "            <li>$\\v{W}^{(l)} = \\v{W}^{(l)} - \\alpha (\\v{D}^{(l)}\\v{A}^{(l-1)})^T$ for all other values of $l$</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>For simplicity (!), I've omitted the biases.</li>\n",
    "    <li>We can see in this more precise version that some of the things that are calculated on the\n",
    "        forward pass get stored.\n",
    "    </li>\n",
    "    <li>These things can then be used on the backward pass.</li>\n",
    "    <li>Similarly, some of the things that are calculated on the backward pass get stored.</li>\n",
    "    <li>These things can then be used to update the weights.</li>\n",
    "    <li>This makes backprop much more efficient than it otherwise would be.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Autodiff</h2>\n",
    "<ul>\n",
    "    <li>Whoa! We have shown the update rules for a <em>particular</em> network and a <em>particular</em> loss function.</li>\n",
    "    <li>We would get different update rules if we changed:\n",
    "        <ul>\n",
    "            <li>the network, e.g. layers other than dense layers (batch normalization layers, convolutional\n",
    "                layers, etc.); and/or\n",
    "            </li>\n",
    "            <li>the loss function.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Happily, we don't have to manually find the partial derivatives all over again.\n",
    "        <ul>\n",
    "            <li>Neural networks consist of layers of operations, each with simple, known derivatives.</li>\n",
    "            <li>Given that the network simply defines a composite function (see previous lecture), the \n",
    "                derivatives for the whole network can be obtained automatically by repeated use of the\n",
    "                <b>chain rule</b>:\n",
    "                <ul>\n",
    "                    <li>To differentiate a function of a function, $y = f(g(x))$, let $u = g(x)$ so that \n",
    "                        $y = f(u)$, then\n",
    "                        $$\\frac{dy}{dx} = \\frac{dy}{du} \\times \\frac{du}{dx}$$\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Hence, modern frameworks such as TensorFlow can compute gradients automatically in the\n",
    "                backpropagation step.\n",
    "            </li>\n",
    "            <li>This is known as <b>autodiff</b> (or, for the way it is used by backprop, <b>reverse mode autodiff</b>).</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "<h1>The Vanishing Gradient Problem</h1>\n",
    "<ul>\n",
    "    <li>Each weight is updated by an amount proportional to the gradient of the loss function with respect to\n",
    "        that weight.\n",
    "    </li>\n",
    "    <li>But if the gradient is very small, the weight doesn't change much.\n",
    "        <ul>\n",
    "            <li>This may prevent the network from converging to a good approximation of\n",
    "                the target function.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We can now see that this is worse for deeper networks.\n",
    "        <ul>\n",
    "            <li>The error signal becomes ever smaller as it is backpropagated.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We look at three solutions:\n",
    "        <ul>\n",
    "            <li>Non-saturating activation functions;</li>\n",
    "            <li>Better initialization;</li>\n",
    "            <li>Batch normalization.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Non-Saturating Activation Functions</h2>\n",
    "<ul>\n",
    "    <li>Certain activation functions, including the sigmoid function, are one cause of\n",
    "        the vanishing gradient problem.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigma(z):\n",
    "    return 1 / (1 + exp(-z))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"Sigmoid function\")\n",
    "xvals = np.linspace(-10, 10, 21)\n",
    "plt.plot(xvals, [sigma(xval) for xval in xvals], color = \"red\")\n",
    "plt.xlabel(\"z\")\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylabel(r\"$\\sigma(z)$\")\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>When the input to this function becomes large (positive or negative), the function \n",
    "        <b>saturates</b> (i.e. becomes very flat).\n",
    "        <ul>\n",
    "            <li>When it saturates, its derivative is extremely close to 0 so there's not much gradient \n",
    "                to propagate back to earlier layers (and what little gradient there is\n",
    "                gets diluted as it propagates back).\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Even when the gradient is at its greatest (when input $z$ is 0 and $\\sigma(z) = 0.5$), it is only 0.25.\n",
    "        <ul>\n",
    "            <li>So in the back propagation, gradients always diminish by a quarter or more.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>This is why we rarely use the sigmoid function as the activation function in the hidden\n",
    "        layers of deep networks.\n",
    "    </li>\n",
    "    <li>Lots of alternatives have been proposed, including the <b>rectified linear unit</b>\n",
    "        activation function, ReLU\n",
    "        $$\\mbox{ReLU}(z) = \\max(0, z)$$\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def relu(z):\n",
    "    return max(z, 0)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"ReLU activation function\")\n",
    "xvals = np.linspace(-10, 10, 21)\n",
    "plt.plot(xvals, [relu(xval) for xval in xvals], color = \"red\")\n",
    "plt.xlabel(\"z\")\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylabel(\"$relu(z)$\")\n",
    "plt.ylim(0.0, 10.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Neurons that use the ReLU activation function have obvious problems too: \n",
    "        <ul>\n",
    "            <li>If their input (weighted sum) is negative, \n",
    "                <ul>\n",
    "                    <li>the output is zero; and</li>\n",
    "                    <li>the gradient is zero;</li>\n",
    "                </ul>\n",
    "                &mdash; and if this is true for all examples in the training set then, in effect, the neuron dies.\n",
    "            </li>\n",
    "            <li>The gradient changes abruptly at $z=0$, which can make Gradient Descent bounce around.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Alternatives to ReLU such as Leaky ReLU, ELU (Exponential Linear Unit) and Scaled ELU have been \n",
    "        proposed, having  at least some non-zero\n",
    "        gradient for negative inputs but they are slower to compute and they introduce further hyperparameters.\n",
    "    </li>\n",
    "    <li>We'll stick with ReLU in this module. Despite its problems, it remains a popular choice.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Better Random Initialization</h2>\n",
    "<ul>\n",
    "    <li>It turns out that vanishing gradients are more likely for certain ways of initializing the weights.</li>\n",
    "    <li>Perhaps the most typical method was to use a normal distribution with mean of 0 and standard deviation \n",
    "        of, e.g., 0.05.\n",
    "    </li>\n",
    "    <li>Better methods have been proposed.\n",
    "        <ul>\n",
    "            <li>One (of many) is Glorot uniform initialization (also called Xavier uniform initialization).</li>\n",
    "            <li>Happily, this is the Keras default.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Batch Normalization</h2>\n",
    "<ul>\n",
    "    <li>We previously studied the usefulness of feature scaling when doing Gradient Descent for,\n",
    "        e.g., linear regression.\n",
    "        <figure>\n",
    "            <img src=\"images/unscaled.png\" style=\"display: inline\" /> \n",
    "            <img src=\"images/scaled.png\" style=\"display: inline\" />\n",
    "        </figure>\n",
    "        &hellip; and we've been doing this to the features in our neural networks too.\n",
    "    </li>\n",
    "    <li>But, if this is a good idea for the inputs to the first hidden layer, why not use the same\n",
    "        idea for the inputs to subsequent layers?\n",
    "        <ul>\n",
    "            <li>In other words, we normalize the activations (outputs) of layer $l$ prior to them being used \n",
    "                as inputs to layer $l+1$.\n",
    "            </li>\n",
    "            <li>This will control the distribution of the values throughout the training process.</li>\n",
    "        </ul>\n",
    "     </li>\n",
    "     <li>This, in essence, is the idea of <b>batch normalization</b>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Other benefits of Batch Normalization</h3>\n",
    "<ul>\n",
    "    <li>Batch Normalization reduces the vanishing gradients problem so much, we can even use saturating\n",
    "        activation functions.\n",
    "    </li>\n",
    "    <li>Training becomes less sensitive to the method used for randomly initializing weights.</li>\n",
    "    <li>Much larger learning rates work (faster convergence) with less risk of divergence.</li>\n",
    "    <li>It acts like a regularizer.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Batch normalization in Keras</h3>\n",
    "<ul>\n",
    "    <li>Just add another layer!</li>\n",
    "    <li>E.g.\n",
    "        <pre>\n",
    "x = Dense(512, activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "        </pre>\n",
    "        This is how we will do batch normalization in CS4618.\n",
    "    </li>\n",
    "    <li>Ignore: in fact, there is some debate about whether we should batch \n",
    "         normalize the activations of\n",
    "        the layer (as above) or the weighted sum, before applying the activation function (as below):\n",
    "        <pre>\n",
    "x = Dense(512, activation=\"linear\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "        </pre>\n",
    "        We'll stick with the former, which is more concise.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exploring Vanishing Gradients with MNIST</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mnist_network(activation, initializer, use_batch_norm):\n",
    "    inputs = Input(shape=(28 * 28,))\n",
    "    x = Rescaling(scale=1./255)(inputs)\n",
    "    x = Dense(512, activation=activation, kernel_initializer=initializer)(x)\n",
    "    if use_batch_norm:\n",
    "        x = BatchNormalization()(x)\n",
    "    outputs = Dense(10, activation=\"softmax\", kernel_initializer=initializer)(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=RMSprop(learning_rate=0.0001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.941100001335144\n",
      "0.9542999863624573\n",
      "0.9412999749183655\n",
      "0.9510999917984009\n",
      "0.974399983882904\n",
      "0.980400025844574\n",
      "0.9758999943733215\n",
      "0.9801999926567078\n"
     ]
    }
   ],
   "source": [
    "networks = [\n",
    "    build_mnist_network(\"sigmoid\", \"random_normal\", False),\n",
    "    build_mnist_network(\"sigmoid\", \"random_normal\", True),\n",
    "    build_mnist_network(\"sigmoid\", \"glorot_uniform\", False),\n",
    "    build_mnist_network(\"sigmoid\", \"glorot_uniform\", True),\n",
    "    build_mnist_network(\"relu\", \"random_normal\", False),\n",
    "    build_mnist_network(\"relu\", \"random_normal\", True),\n",
    "    build_mnist_network(\"relu\", \"glorot_uniform\", False),\n",
    "    build_mnist_network(\"relu\", \"glorot_uniform\", True)\n",
    "]\n",
    "\n",
    "for network in networks:\n",
    "    network.fit(mnist_x_train, mnist_y_train, epochs=10, batch_size=32, verbose=0)\n",
    "    test_loss, test_acc = network.evaluate(mnist_x_test, mnist_y_test, verbose=0)\n",
    "    print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>We shouldn't read too much into the results above.\n",
    "        The ideas in this lecture apply to <em>deep</em> networks.\n",
    "        This network is not deep.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Hyperparameter Tuning</h1>\n",
    "<ul>\n",
    "    <li>First an observation about validation sets:\n",
    "        <ul>\n",
    "            <li>When using neural networks, we typically have a large dataset.</li>\n",
    "            <li>Hence, we use <i>holdout</i> to split the dataset into train, validation and test sets.</li>\n",
    "            <li>If you have a smaller dataset, where you can afford to split off a test set, but you cannot afford to split off a validation set, then you might want to use $k$-fold cross-validation, as we did in our scikit-learn examples.</li>\n",
    "            <li>Keras does not have any in-built cross-validation functions. You'd have to write your own. Or copy the code from somewhere such as Chollet's book or an online tutorial.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Second, an observation about hyperparameter tuning:\n",
    "        <ul>\n",
    "            <li>Hyperparameter tuning involves training lots of different models with different values for the hyperparameters, and comparing their performance on the validation data.</li>\n",
    "            <li>This is often a problem with neural networks: training can be slow (due to large datasets and models that have many parameters) and there can be quite a lot of hyperparameters. Hence, training lots of different models for hyperparameter tuning is very time-consuming.</li>\n",
    "            <li>People often  don't bother! They just pick hyperparameter values out of their heads, or accept the Keras defaults.</li>\n",
    "            <li>If you can afford to be more systematic, there is a separate library called KerasTuner. It automates hyperparameter tuning, similar to what we saw in scikit-learn.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Appendix</h1>\n",
    "<h2>Some of the details of Batch Normalization</h2>\n",
    "<ul>\n",
    "    <li>(No need to learn these details &mdash; and there are some notation abuses anyway!)</li>\n",
    "    <li>In summary, for a given layer, it standardizes the outputs of the neurons (subtract the mean, divide by\n",
    "        the standard deviation), then it scales the result and adds an offset.\n",
    "    </li>\n",
    "    <li>It standardizes the output of a previous activation layer by subtracting the batch mean and dividing\n",
    "        by the batch standard deviation:\n",
    "        <ul>\n",
    "            <li>Let $B$ be the batch of examples of size $m_B$.</li>\n",
    "            <li>The mean of a batch of activation values $\\mu_B = \\frac{1}{m_B}\\sum_{i=1}^{m_B} a^{(i)}$.</li>\n",
    "            <li>Their standard deviation $\\sigma^2_B = \\frac{1}{m_B}\\sum_{i=1}^{m_B}(a^{(i)} - \\mu_B)^2$.</li>\n",
    "            <li>Standardize them: $a^{(i)}_{\\mbox{norm}} = \\frac{a^{(i)} - \\mu_B}{\\sqrt{\\sigma^2_B + \\epsilon}}$,\n",
    "                where\n",
    "                $\\epsilon$ is a small value to avoid division-by-zero problems.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>But, while we want the different units to have comparable activation values (achieved by\n",
    "        standardizing), we don't necessarily want them to have mean 0 and standard deviation 1.\n",
    "    </li>\n",
    "    <li>So, we multiply by a scaling factor for the layer and add an offset for the layer.\n",
    "        <ul>\n",
    "            <li>Scale and add offset: $\\tilde{a}^{(i)} = \\gamma a^{(i)}_{\\mbox{norm}} + \\beta$\n",
    "            <li>$\\gamma$ and $\\beta$ are parameters and so they are also learned by the Gradient Descent.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "<!--\n",
    "<h2>Learning Rate</h2>\n",
    "<ul>\n",
    "    <li>Among many hyperparameters, perhaps the most important is the learning rate.\n",
    "        <ul>\n",
    "            <li>This can affect whether training converges or diverges.</li>\n",
    "            <li>By changining the learning rate using a <b>learning rate schedule</b> (as we do in\n",
    "                <b>simulated annealing</b>), we can improve convergence of SGD and \n",
    "                Mini-Batch GD, and maybe avoid getting stuck in some local minima.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Here we'll discuss two things: (a) one way to choose a learning rate and (b) one\n",
    "        learning schedule.\n",
    "    </li>\n",
    "    <li>The techniques we look at have been popularized by their use in the online\n",
    "        <a href=\"https://www.fast.ai/\">FastAI course</a>.\n",
    "    </li>\n",
    "</ul>\n",
    "<h3>Plotting loss</h3>\n",
    "<ul>\n",
    "    <li>Two notes about the Keras <code>fit</code> method:\n",
    "        <ul>\n",
    "            <li>We can specify a validation dataset (<code>validation_data</code>) or request that\n",
    "                the dataset be split (<code>validation_split</code>).\n",
    "            </li>\n",
    "            <li>It returns a <code>History</code> object whose <code>history</code> attribute\n",
    "                records, for each epoch, the training losses and validation losses, and also the\n",
    "                values for any metrics requested in the <code>compile</code> method.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>So we can choose a learning rate and plot losses. \n",
    "        <ul>\n",
    "            <li>If loss comes down too slowly, the learning rate may be too small.</li>\n",
    "            <li>If loss starts to go up, then the learning rate may be too large (divergence).</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "def build_mnist_model():\n",
    "    inputs = Input(shape=(28 * 28,))\n",
    "    x = Rescaling(scale=1./255)(inputs)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    outputs = Dense(10, activation=\"softmax\")(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=RMSprop(learning_rate=0.0001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "mnist_model = build_mnist_model()\n",
    "\n",
    "history = mnist_model.fit(mnist_x_train, mnist_y_train, epochs=25, batch_size=32, validation_split=0.2, verbose=0)\n",
    "<ul>\n",
    "    <li>We can plot training error and validation error (loss) on the $y$-axis and epochs on the $x$-axis\n",
    "        to see our progress.\n",
    "    </li>\n",
    "    <li>We will call this kind of plot a <b>training curve</b>.\n",
    "        <ul>\n",
    "            <li>How do the $x$-axes differ in a <b>validation curve</b>, <b>learning curve</b>\n",
    "                and <b>training curve</b>?\n",
    "            </li>\n",
    "            <li>What is each used for?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    " </ul>\n",
    " pd.DataFrame(history.history).plot()\n",
    " <ul>\n",
    "    <li>(There is an explanaton for why validation loss can be initially lower than training loss: they are\n",
    "        half an epoch out-of-step.)\n",
    "    </li>\n",
    "    <li>Here, the learning rate we chose seems OK.</li>\n",
    "    <li>If it weren't, we could change it, retrain and re-plot.</li>\n",
    "    <li>But this is hit-and-miss. One alternative is grid search or randomized search, but these are \n",
    "        expensive.\n",
    "    </li>\n",
    "</ul>\n",
    "<h3>Finding a learning rate</h3>\n",
    "<ul>\n",
    "    <li>A popular alternative to hit-and-miss and grid search/randomized search is:\n",
    "        <ul>\n",
    "            <li>Train for just a few epochs, e.g. 5.</li>\n",
    "            <li>Start with a very low learning rate (e.g. 1e-10), then a  higher one, \n",
    "                then a higher one, and so on up to some high value (e.g. 10).\n",
    "            </li>\n",
    "            <li>Change the learning rate after each <em>batch</em>.</li>\n",
    "            <li>Plot the loss ($y$-axis) against learning rate ($x$-axis).</li>\n",
    "            <li>You should see loss decreasing slowly, then dropping sharply, and then increasing\n",
    "                sharply.\n",
    "            </li>\n",
    "            <li>The best learning rate is just before loss starts to rise: rule-of-thumb is 10 times\n",
    "                smaller than the bottom.\n",
    "            </li>\n",
    "        </ul>\n",
    "        From now on, use this as your learning rate.\n",
    "    </li>\n",
    "</ul>   \n",
    "mnist_model = build_mnist_model()\n",
    "\n",
    "lr_finder = LearningRateFinder(mnist_model)\n",
    "\n",
    "lr_finder.find(trainData=(mnist_x_train, mnist_y_train), \n",
    "               startLR=1e-10, endLR=1e+1, \n",
    "               epochs=5, batchSize=32, verbose=0)\n",
    "\n",
    "lr_finder.plot_loss(skipBegin=10, skipEnd=1, title=\"\")\n",
    "<ul>\n",
    "    <li>This seems to be recommending 10e-2 or 10e-3 (it can vary from\n",
    "        run to run).\n",
    "    </li>\n",
    "</ul>\n",
    "<h3>1cycle scheduling</h3>\n",
    "<ul>\n",
    "    <li>If an optimizer uses a learning rate schedule, then most likely the schedule reduces the\n",
    "        learning rate over time.\n",
    "    </li>\n",
    "    <li>But reducing the learning rate may nevertheless leave us stuck on plateaus or in local minima.</li>\n",
    "    <li>1cycle scheduling, popularized by FastAI, uses two learning rates:\n",
    "        <ul>\n",
    "            <li>minimum learning rate; and</li>\n",
    "            <li>maximum learning rate.</li>\n",
    "            Simply, we change the learning rate after every batch: it grows from minimum to maximum,\n",
    "            then from maximum to minimum, then from\n",
    "            minimum to maximum, and so on.\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>What should you use as the minimum and maximum learning rates?\n",
    "        <ul>\n",
    "            <li>One suggestion is to use the plot from the learning rate finder above: just after it starts\n",
    "                falling, and just before it starts growing.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "mnist_network = build_mnist_model()\n",
    "\n",
    "batch_size = 32\n",
    "step_size = 6 * mnist_x_train.shape[0] // 32\n",
    "\n",
    "clr = CyclicLR(base_lr=10e-3, max_lr=10e-1, step_size=step_size)\n",
    "\n",
    "history = mnist_model.fit(mnist_x_train, mnist_y_train, epochs=25, batch_size=batch_size, validation_split=0.2, \n",
    "                          callbacks=[clr], verbose=0)\n",
    "<ul>\n",
    "    <li>The recommendation is that <code>step_size</code> is some multiple of (e.g. $6\\times$) the number of\n",
    "        batches per epoch.\n",
    "    </li>\n",
    "</ul>\n",
    "test_loss, test_acc = mnist_model.evaluate(mnist_x_test, mnist_y_test)\n",
    "test_acc\n",
    "pd.DataFrame(history.history).plot()\n",
    "<ul>\n",
    "    <li>We can even see how the learning rate changes:</li>\n",
    "</ul>\n",
    "plt.plot(clr.history[\"lr\"])\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
